{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rCT0KQzLV4Y"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "import psutil\n",
    "import subprocess\n",
    "import platform\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.utils import image_dataset_from_directory\n",
    "from tensorflow.data import Options\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db341ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth enabled.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3gnERLOeGV_O",
    "outputId": "e457719c-f736-40cf-c5cb-100833cb2b95"
   },
   "outputs": [],
   "source": [
    "# Caminho local para a pasta raiz do projeto\n",
    "root_path = \"./\"  \n",
    "\n",
    "# Listar diret√≥rios no caminho raiz\n",
    "print(\"üìÅ Diret√≥rios no caminho raiz:\")\n",
    "print(os.listdir(root_path))\n",
    "\n",
    "# Verificar conte√∫do de um caminho espec√≠fico\n",
    "specific_path = os.path.join(root_path, \"garbage-noaug-70-15-15\")\n",
    "if os.path.exists(specific_path):\n",
    "    print(f\"\\nüìÅ Conte√∫do de {specific_path}:\")\n",
    "    print(os.listdir(specific_path))\n",
    "else:\n",
    "    print(f\"\\n‚ùå Caminho {specific_path} n√£o existe\")\n",
    "\n",
    "# Fun√ß√£o para listar diret√≥rios com profundidade\n",
    "def list_dirs(path, indent=0):\n",
    "    for item in os.listdir(path):\n",
    "        full_path = os.path.join(path, item)\n",
    "        if os.path.isdir(full_path):\n",
    "            print(\" \" * indent + \"üìÅ \" + item)\n",
    "            if indent < 4:\n",
    "                list_dirs(full_path, indent + 2)\n",
    "        else:\n",
    "            print(\" \" * indent + \"üìÑ \" + item)\n",
    "\n",
    "# Explorar estrutura de diret√≥rios\n",
    "print(\"\\nüìÇ Estrutura de diret√≥rios:\")\n",
    "list_dirs(root_path, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved Metal GPU detection for Apple Silicon\n",
    "try:\n",
    "    # First try looking for GPU devices (newer TF versions label Metal as GPU)\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if len(gpus) > 0:\n",
    "        print(f\"Found {len(gpus)} GPU device(s)\")\n",
    "        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(\"GPU acceleration enabled (Metal)\")\n",
    "    # If no GPU found, try looking specifically for MPS devices\n",
    "    elif hasattr(tf.config, 'list_physical_devices') and len(tf.config.list_physical_devices('MPS')) > 0:\n",
    "        mps_devices = tf.config.list_physical_devices('MPS')\n",
    "        tf.config.experimental.set_visible_devices(mps_devices[0], 'MPS')\n",
    "        print(\"MPS (Metal) device enabled\")\n",
    "    else:\n",
    "        print(\"No GPU or MPS device found, using CPU\")\n",
    "        \n",
    "    # Verify what device is being used\n",
    "    print(\"\\nDevice being used:\", tf.config.get_visible_devices())\n",
    "    \n",
    "    # Test with a simple operation to confirm GPU usage\n",
    "    with tf.device('/GPU:0'):\n",
    "        a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "        b = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "        c = tf.matmul(a, b)\n",
    "        print(\"Matrix multiplication result:\", c)\n",
    "        print(\"GPU test successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error setting up GPU: {e}\")\n",
    "    print(\"Falling back to CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable mixed precision (faster on GPU)\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "set_global_policy('mixed_float16')  # Use FP16 instead of FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QM_Q87BIO7Bo",
    "outputId": "8bf53a53-57b1-45bd-d7ac-7e0b0aa95b05"
   },
   "outputs": [],
   "source": [
    "train_dir = specific_path + \"/train\"\n",
    "validation_dir = specific_path + \"/valid\"\n",
    "test_dir = specific_path + \"/test\"\n",
    "\n",
    "# Images are 640, but 224 is way faster for training\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    validation_dir,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Aumentar o dataset de treinamento com t√©cnicas de data augmentation (das classes minorit√°rias)\n",
    "def augment_image(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "    image = tf.image.random_contrast(image, 0.8, 1.2)\n",
    "    image = tf.image.random_saturation(image, 0.8, 1.2)\n",
    "    image = tf.image.random_hue(image, 0.05)\n",
    "    return image, label\n",
    "\n",
    "# Definir classes minorit√°rias\n",
    "minority_classes = [0, 1, 5, 9]  # battery, biological, metal, trash\n",
    "\n",
    "def augment_conditionally(image, label):\n",
    "    return tf.cond(\n",
    "        tf.reduce_any([tf.equal(label, tf.constant(c)) for c in minority_classes]),\n",
    "        lambda: augment_image(image, label),\n",
    "        lambda: (image, label)\n",
    "    )\n",
    "\n",
    "train_dataset_aug = train_dataset.map(augment_conditionally, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "\n",
    "train_dataset_pref = train_dataset_aug.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "validation_dataset_pref = validation_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_dataset_pref = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "options = Options()\n",
    "options.experimental_optimization.parallel_batch = False\n",
    "train_dataset_pref = train_dataset_pref.with_options(options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eYyaeQfoQP3B",
    "outputId": "cca51f1e-b6b2-4db3-8ccd-dabaf62b3556"
   },
   "outputs": [],
   "source": [
    "class_counts = {}\n",
    "for _, labels in train_dataset_aug:\n",
    "    for label in labels.numpy():\n",
    "        class_counts[label] = class_counts.get(label, 0) + 1\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'class': [train_dataset.class_names[i] for i in range(len(train_dataset.class_names))],\n",
    "    'count': [class_counts.get(i, 0) for i in range(len(train_dataset.class_names))]\n",
    "})\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5CqMUaOEQawK"
   },
   "outputs": [],
   "source": [
    "# Definir a arquitetura da CNN com BatchNormalization antes da ativa√ß√£o\n",
    "inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "\n",
    "# Primeira camada convolucional\n",
    "x = layers.Conv2D(32, 3, padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.MaxPooling2D()(x)\n",
    "\n",
    "# Segunda camada convolucional\n",
    "x = layers.Conv2D(64, 3, padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.MaxPooling2D()(x)\n",
    "\n",
    "# Terceira camada convolucional\n",
    "x = layers.Conv2D(128, 3, padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.MaxPooling2D()(x)\n",
    "\n",
    "# Camadas densas\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "\n",
    "# Camada de sa√≠da\n",
    "outputs = layers.Dense(len(train_dataset.class_names), activation='softmax')(x)\n",
    "\n",
    "# Criar o modelo\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Otimizador\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Compilar o modelo\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Resumo do modelo\n",
    "model.summary()\n",
    "\n",
    "# Callback para early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to your training cell\n",
    "class TimeoutCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, max_time_mins=2):\n",
    "        super().__init__()\n",
    "        self.max_time_sec = max_time_mins * 60\n",
    "        self.start_time = time.time()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        elapsed = time.time() - self.start_time\n",
    "        if elapsed > self.max_time_sec:\n",
    "            print(f\"\\nReached time limit ({self.max_time_sec/3600:.1f}h). Stopping training.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Maximum 20 minutes of training to prevent overheating\n",
    "timeout_cb = TimeoutCallback(max_time_mins=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WURl_OhwQg_E",
    "outputId": "3484b4a3-12a5-439a-cf55-da9bd59f90ea"
   },
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy', \n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"models/model_checkpoint.keras\", \n",
    "    save_best_only=True,\n",
    "    monitor=\"val_accuracy\"\n",
    ")\n",
    "\n",
    "# Add more callbacks for better training\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Calculate class weights\n",
    "total = sum(class_counts.values())\n",
    "class_weight = {i: total/count for i, count in class_counts.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResourceMonitorCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, check_interval=1):\n",
    "        super().__init__()\n",
    "        self.check_interval = check_interval\n",
    "        self.epoch_count = 0\n",
    "        self.is_mac = platform.system() == 'Darwin'\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch_count += 1\n",
    "        if self.epoch_count % self.check_interval == 0:\n",
    "            # Get basic info\n",
    "            cpu_percent = psutil.cpu_percent(interval=0.5)\n",
    "            memory = psutil.virtual_memory()\n",
    "            mem_used = f\"{memory.percent}% ({memory.used / 1024**3:.1f}GB)\"\n",
    "            \n",
    "            # Temperature check - simplified\n",
    "            temp = \"N/A\"\n",
    "            if self.is_mac:\n",
    "                try:\n",
    "                    # Try thermal level from pmset (no sudo needed)\n",
    "                    result = subprocess.run(['pmset', '-g', 'therm'], capture_output=True, text=True)\n",
    "                    if \"CPU_Thermal_level\" in result.stdout:\n",
    "                        temp = result.stdout.strip()\n",
    "                except: pass\n",
    "            \n",
    "            # Simplified output\n",
    "            print(f\"\\n[Epoch {epoch}] CPU: {cpu_percent}% | Memory: {mem_used}\")\n",
    "            print(f\"Thermal: {temp}\")\n",
    "            print(f\"GPU: {'Active' if self.is_mac else 'Unknown'}\")\n",
    "\n",
    "# Create monitor that checks every epoch\n",
    "resource_monitor = ResourceMonitorCallback(check_interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINN!!!!\n",
    "history = model.fit(\n",
    "    train_dataset_pref,\n",
    "    validation_data=validation_dataset_pref,\n",
    "    epochs=20,\n",
    "    class_weight=class_weight, \n",
    "    callbacks=[early_stopping, reduce_lr, checkpoint_cb, timeout_cb, resource_monitor]\n",
    ")\n",
    "\n",
    "# Save the entire model (architecture + weights + optimizer state)\n",
    "model.save(\"models/garbage_classifier_model_early_stopping_aug.keras\")  \n",
    "model.save_weights(\"models/garbage_classifier_early_stopping_aug.weights.h5\")\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43mAwfAPQj3J"
   },
   "outputs": [],
   "source": [
    "# Corrected plotting code for newer TensorFlow versions\n",
    "accuracy = history.history['accuracy']\n",
    "val_accuracy = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, accuracy, 'bo-', label='Training accuracy')\n",
    "plt.plot(epochs, val_accuracy, 'ro-', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, loss, 'bo-', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'ro-', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test dataset\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "\n",
    "# Visualize some predictions\n",
    "# Get class names from your dataset\n",
    "class_names = train_dataset.class_names\n",
    "print(\"Classes:\", class_names)\n",
    "\n",
    "# Function to show predictions for a batch of images\n",
    "plt.figure(figsize=(12, 12))\n",
    "for images, labels in test_dataset.take(1):\n",
    "    predictions = model.predict(images)\n",
    "    pred_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    for i in range(24):\n",
    "        plt.subplot(6, 4, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        \n",
    "        correct = labels[i] == pred_classes[i]\n",
    "        color = \"green\" if correct else \"red\"\n",
    "        \n",
    "        plt.title(f\"True: {class_names[labels[i]]}\\nPred: {class_names[pred_classes[i]]}\", \n",
    "                 color=color)\n",
    "        plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
