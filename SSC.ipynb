{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/lucas-remigio/ssc-ai-garbage-detection/blob/main/SSC.ipynb",
      "authorship_tag": "ABX9TyPDEo6L5iuyGdOMrWQERH9Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucas-remigio/ssc-ai-garbage-detection/blob/main/SSC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9rCT0KQzLV4Y"
      },
      "outputs": [],
      "source": [
        "from keras.utils import image_dataset_from_directory\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import models\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# List directories in your Google Drive root\n",
        "print(\"Directories in Drive root:\")\n",
        "root_path = \"/content/drive/MyDrive\"\n",
        "print(os.listdir(root_path))\n",
        "\n",
        "# /content/drive/MyDrive/Colab Notebooks/garbage-70-15-15\n",
        "\n",
        "# If you want to check a specific path\n",
        "specific_path = \"/content/drive/MyDrive/Colab Notebooks/garbage-70-15-15\"\n",
        "if os.path.exists(specific_path):\n",
        "    print(f\"\\nContents of {specific_path}:\")\n",
        "    print(os.listdir(specific_path))\n",
        "else:\n",
        "    print(f\"\\nPath {specific_path} does not exist\")\n",
        "\n",
        "# Function to list directories with a specific depth\n",
        "def list_dirs(path, indent=0):\n",
        "    for item in os.listdir(path):\n",
        "        full_path = os.path.join(path, item)\n",
        "        if os.path.isdir(full_path):\n",
        "            print(\" \" * indent + \"üìÅ \" + item)\n",
        "            if indent < 4:  # Limit recursion depth\n",
        "                list_dirs(full_path, indent + 2)\n",
        "        else:\n",
        "            print(\" \" * indent + \"üìÑ \" + item)\n",
        "\n",
        "# Use this to explore your Drive structure\n",
        "print(\"\\nDirectory structure:\")\n",
        "list_dirs(root_path, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gnERLOeGV_O",
        "outputId": "e457719c-f736-40cf-c5cb-100833cb2b95"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Directories in Drive root:\n",
            "['Colab Notebooks']\n",
            "\n",
            "Contents of /content/drive/MyDrive/Colab Notebooks/garbage-70-15-15:\n",
            "['test', 'train', 'valid', 'README.roboflow.txt', 'README.dataset.txt']\n",
            "\n",
            "Directory structure:\n",
            "üìÅ Colab Notebooks\n",
            "  üìÑ SSC.ipynb\n",
            "  üìÅ garbage-dataset\n",
            "    üìÑ .DS_Store\n",
            "    üìÅ biological\n",
            "    üìÅ paper\n",
            "    üìÅ metal\n",
            "    üìÅ trash\n",
            "    üìÅ glass\n",
            "    üìÅ clothes\n",
            "    üìÅ cardboard\n",
            "    üìÅ battery\n",
            "    üìÅ shoes\n",
            "    üìÅ plastic\n",
            "  üìÅ garbage-70-15-15\n",
            "    üìÅ test\n",
            "    üìÅ train\n",
            "    üìÅ valid\n",
            "    üìÑ README.roboflow.txt\n",
            "    üìÑ README.dataset.txt\n",
            "  üìÅ garbage-noaug-70-15-15\n",
            "    üìÑ .DS_Store\n",
            "    üìÅ valid\n",
            "    üìÅ test\n",
            "    üìÅ train\n",
            "    üìÑ README.roboflow.txt\n",
            "    üìÑ README.dataset.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = specific_path + \"/train\"\n",
        "validation_dir = specific_path + \"/valid\"\n",
        "test_dir = specific_path + \"/test\"\n",
        "\n",
        "# Images are 640, but 224 is way faster for training\n",
        "IMG_SIZE = 224\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    train_dir,\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=32\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QM_Q87BIO7Bo",
        "outputId": "8bf53a53-57b1-45bd-d7ac-7e0b0aa95b05"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 41648 files belonging to 10 classes.\n",
            "Found 2995 files belonging to 10 classes.\n",
            "Found 2995 files belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for data_batch, labels_batch in train_dataset:\n",
        "    print('data batch shape:', data_batch.shape)\n",
        "    print('labels batch shape:', labels_batch.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYyaeQfoQP3B",
        "outputId": "cca51f1e-b6b2-4db3-8ccd-dabaf62b3556"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data batch shape: (32, 224, 224, 3)\n",
            "labels batch shape: (32,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Flatten()(x)\n",
        "# Dropout for better generalization\n",
        "x = layers.Dropout(0.5)(x)\n",
        "x = layers.Dense(512, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4),\n",
        "    metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "5CqMUaOEQawK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=validation_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WURl_OhwQg_E",
        "outputId": "3484b4a3-12a5-439a-cf55-da9bd59f90ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m   2/1302\u001b[0m \u001b[37m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[1m1:25:16\u001b[0m 4s/step - accuracy: 0.2344 - loss: 2.2434 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo' , label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "43mAwfAPQj3J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}