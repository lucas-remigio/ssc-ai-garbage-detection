# -*- coding: utf-8 -*-
"""SSC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/lucas-remigio/ssc-ai-garbage-detection/blob/main/SSC.ipynb
"""

from keras.utils import image_dataset_from_directory
import matplotlib.pyplot as plt
from tensorflow import keras
from keras import layers
from keras import models
import tensorflow as tf

from google.colab import drive
import os

# Mount Google Drive
drive.mount('/content/drive')

# List directories in your Google Drive root
print("Directories in Drive root:")
root_path = "/content/drive/MyDrive"
print(os.listdir(root_path))

# /content/drive/MyDrive/Colab Notebooks/garbage-70-15-15

# If you want to check a specific path
specific_path = "/content/drive/MyDrive/Colab Notebooks/garbage-70-15-15"
if os.path.exists(specific_path):
    print(f"\nContents of {specific_path}:")
    print(os.listdir(specific_path))
else:
    print(f"\nPath {specific_path} does not exist")

# Function to list directories with a specific depth
def list_dirs(path, indent=0):
    for item in os.listdir(path):
        full_path = os.path.join(path, item)
        if os.path.isdir(full_path):
            print(" " * indent + "üìÅ " + item)
            if indent < 4:  # Limit recursion depth
                list_dirs(full_path, indent + 2)
        else:
            print(" " * indent + "üìÑ " + item)

# Use this to explore your Drive structure
print("\nDirectory structure:")
list_dirs(root_path, 0)

train_dir = specific_path + "/train"
validation_dir = specific_path + "/valid"
test_dir = specific_path + "/test"

# Images are 640, but 224 is way faster for training
IMG_SIZE = 224

train_dataset = image_dataset_from_directory(
    train_dir,
    image_size=(IMG_SIZE, IMG_SIZE),
    batch_size=32
)

validation_dataset = image_dataset_from_directory(
    validation_dir,
    image_size=(IMG_SIZE, IMG_SIZE),
    batch_size=32
)

test_dataset = image_dataset_from_directory(
    test_dir,
    image_size=(IMG_SIZE, IMG_SIZE),
    batch_size=32
)

for data_batch, labels_batch in train_dataset:
    print('data batch shape:', data_batch.shape)
    print('labels batch shape:', labels_batch.shape)
    break

inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
x = layers.Rescaling(1./255)(inputs)
x = layers.Conv2D(filters=32, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=128, kernel_size=3, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Flatten()(x)
# Dropout for better generalization
x = layers.Dropout(0.5)(x)
x = layers.Dense(512, activation="relu")(x)
outputs = layers.Dense(10, activation="softmax")(x)
model = keras.Model(inputs=inputs, outputs=outputs)
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4),
    metrics=['accuracy'])

history = model.fit(
    train_dataset,
    epochs=30,
    validation_data=validation_dataset)

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)
plt.plot(epochs, acc, 'bo', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()
plt.figure()
plt.plot(epochs, loss, 'bo' , label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()
plt.show()